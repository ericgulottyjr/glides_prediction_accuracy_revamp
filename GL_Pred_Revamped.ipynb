{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Define the output directory structure as before\n",
    "notebook_directory = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "raw_output_directory = os.path.join(notebook_directory, 'temp', 'raw')\n",
    "\n",
    "# Create the output directories if they don't exist\n",
    "os.makedirs(raw_output_directory, exist_ok=True)\n",
    "\n",
    "# Define the current date\n",
    "today_date = datetime.now().strftime('%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a global variable that represents the lines of interest (will be GL in the future)\n",
    "target_lines = ['Red', 'Orange']\n",
    "\n",
    "# Define the required dictionary which specifies the relationship between stop_ids and stop names\n",
    "\"\"\"stop_dict = {\n",
    "    'Boston College': ['70106', '70107'],\n",
    "    'Cleveland Circle': ['70237', '70238'],\n",
    "    'Riverside': ['70160', '70161'],\n",
    "    'Union Square': ['70503', '70504'],\n",
    "    'Medford Tufts': ['70511', '70512'],\n",
    "    'Reservoir': ['70174', '70175']\n",
    "}\"\"\"\n",
    "\n",
    "stop_dict = {\n",
    "    'Ashmont' : ['70093','70094'],\n",
    "    'Braintree' : ['70105'],\n",
    "    'Forest Hills' : ['70001'],\n",
    "    'Oak Grove' : ['70036']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify the S3 bucket name\n",
    "bucket_name = 'mbta-gtfs-s3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_objects_with_prefix(prefix):\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "    objects = []\n",
    "    for page in pages:\n",
    "        objects.extend(page.get('Contents', []))\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def download_file(bucket_name, file_key, local_file_path):\n",
    "    if not os.path.exists(os.path.dirname(local_file_path)):\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "    s3_client.download_file(Bucket=bucket_name, Key=file_key, Filename=local_file_path)\n",
    "    print(f\"Downloaded: {file_key} -> {local_file_path}\")\n",
    "\n",
    "def download_files(start_date_str, end_date_str=None):\n",
    "    start_date = datetime.strptime(start_date_str, '%Y/%m/%d')\n",
    "    end_date = datetime.strptime(end_date_str, '%Y/%m/%d') if end_date_str else datetime.now()\n",
    "\n",
    "    file_types = ['realtime_TripUpdates_enhanced', 'realtime_VehiclePositions_enhanced']\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        prefix = current_date.strftime('%Y/%m/%d/')\n",
    "        objects = list_objects_with_prefix(prefix)\n",
    "\n",
    "        # Prepare a list to keep track of tasks\n",
    "        tasks = []\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for file_type in file_types:\n",
    "                for obj in objects:\n",
    "                    file_key = obj['Key']\n",
    "\n",
    "                    if file_type in file_key:\n",
    "                        file_name = os.path.basename(file_key)\n",
    "                        target_dir = os.path.join('temp/raw', prefix)\n",
    "                        local_file_path = os.path.join(target_dir, file_name)\n",
    "\n",
    "                        if not os.path.exists(local_file_path):\n",
    "                            # Schedule the download task\n",
    "                            tasks.append(executor.submit(download_file, bucket_name, file_key, local_file_path))\n",
    "\n",
    "            # Wait for all tasks to complete\n",
    "            for task in tasks:\n",
    "                task.result()  # This will re-raise any exceptions caught during the task execution\n",
    "\n",
    "        current_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_folder(bucket_name, s3_folder, local_dir=None):\n",
    "    \"\"\"\n",
    "    Download the contents of a folder directory\n",
    "    Args:\n",
    "        bucket_name: the name of the s3 bucket\n",
    "        s3_folder: the folder path in the s3 bucket\n",
    "        local_dir: a relative or absolute directory path in the local file system\n",
    "    \"\"\"\n",
    "    bucket = s3_client.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.filter(Prefix=s3_folder):\n",
    "        target = obj.key if local_dir is None \\\n",
    "            else os.path.join(local_dir, os.path.relpath(obj.key, s3_folder))\n",
    "        if not os.path.exists(os.path.dirname(target)):\n",
    "            os.makedirs(os.path.dirname(target))\n",
    "        if obj.key[-1] == '/':\n",
    "            continue\n",
    "        bucket.download_file(obj.key, target)\n",
    "\n",
    "#download_s3_folder(bucket_name, '/2024/02/15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_trip_updates_for_stops(raw_directory='temp/raw', parsed_directory='temp/parsed_TU'):\n",
    "    # Initialize the dataframes dictionary to store DataFrames for each stop\n",
    "    data = {stop: [] for stop in stop_dict}\n",
    "\n",
    "    # Iterate through directories and process files\n",
    "    for year in os.listdir(raw_directory):\n",
    "        year_directory = os.path.join(raw_directory, year)\n",
    "        if os.path.isdir(year_directory):\n",
    "            for month in os.listdir(year_directory):\n",
    "                month_directory = os.path.join(year_directory, month)\n",
    "                if os.path.isdir(month_directory):\n",
    "                    for day in os.listdir(month_directory):\n",
    "                        day_directory = os.path.join(month_directory, day)\n",
    "                        if os.path.isdir(day_directory):\n",
    "                            for root, dirs, files in os.walk(day_directory):\n",
    "                                for file_name in files:\n",
    "                                    if file_name.endswith('.gz'):\n",
    "                                        file_path = os.path.join(day_directory, file_name)\n",
    "                                        with gzip.open(file_path, 'rb') as gz_file:\n",
    "                                            json_content = gz_file.read()\n",
    "                                            json_data = json.loads(json_content)\n",
    "                                            \n",
    "                                            if 'entity' in json_data:\n",
    "                                                for entity in json_data['entity']:\n",
    "                                                    if 'trip_update' in entity and 'stop_time_update' in entity['trip_update']:\n",
    "                                                        stop_time_updates = entity['trip_update']['stop_time_update']\n",
    "                                                        for stop_update in stop_time_updates:\n",
    "                                                            if 'departure' in stop_update and 'stop_id' in stop_update:\n",
    "                                                                stop_id = stop_update['stop_id']\n",
    "                                                                for stop_name, ids in stop_dict.items():\n",
    "                                                                    if stop_id in ids:\n",
    "                                                                        departure_time = pd.to_datetime(stop_update['departure']['time'], unit='s')\n",
    "                                                                        file_time = file_name.split('_')[0].split('T')[1].replace('Z', '').replace('/', ':')\n",
    "                                                                        datetime_str = f\"{year}-{month}-{day} {file_time}\"\n",
    "                                                                        datetime_obj = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "                                                                        current_time = datetime_obj - timedelta(hours=5)\n",
    "                                                                        departure_time_adj = departure_time - timedelta(hours=5)\n",
    "\n",
    "                                                                        new_row = {'id': entity['id'],\n",
    "                                                                                   'stop_id': stop_id,\n",
    "                                                                                   'file_timestamp': datetime_obj,\n",
    "                                                                                   'current_time': current_time,\n",
    "                                                                                   'departure_time': departure_time,\n",
    "                                                                                   'departure_time_adj': departure_time_adj}\n",
    "                                                                        data[stop_name].append(new_row)\n",
    "\n",
    "    # Convert lists to DataFrames, sort by current_time, and save as CSV\n",
    "    os.makedirs(parsed_directory, exist_ok=True)\n",
    "    for stop_name, rows in data.items():\n",
    "        df = pd.DataFrame(rows, columns=['id', 'stop_id', 'file_timestamp', 'current_time', 'departure_time', 'departure_time_adj'])\n",
    "        # Sorting by current_time\n",
    "        df = df.sort_values(by='current_time')\n",
    "        csv_file_path = os.path.join(parsed_directory, f\"{stop_name}.csv\")\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Saved DataFrame for {stop_name}, sorted by current time, to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_files(raw_directory='temp/raw', parsed_TU_directory='temp/parsed_TU', parsed_VP_directory='temp/parsed_VP'):\n",
    "    # Initialize the data dictionaries to store data\n",
    "    data_TU = {stop: [] for stop in stop_dict}\n",
    "    data_VP = {}\n",
    "    \n",
    "    # Iterate through directories and process files\n",
    "    for year in os.listdir(raw_directory):\n",
    "        year_directory = os.path.join(raw_directory, year)\n",
    "        if os.path.isdir(year_directory):\n",
    "            for month in os.listdir(year_directory):\n",
    "                month_directory = os.path.join(year_directory, month)\n",
    "                if os.path.isdir(month_directory):\n",
    "                    for day in os.listdir(month_directory):\n",
    "                        day_directory = os.path.join(month_directory, day)\n",
    "                        if os.path.isdir(day_directory):\n",
    "                            for root, dirs, files in os.walk(day_directory):\n",
    "                                for file_name in files:\n",
    "                                    if file_name.endswith('.gz'):\n",
    "                                        file_path = os.path.join(day_directory, file_name)\n",
    "                                        with gzip.open(file_path, 'rb') as gz_file:\n",
    "                                            json_content = gz_file.read()\n",
    "                                            json_data = json.loads(json_content)\n",
    "                                            \n",
    "                                            # Determine the type of file and process accordingly\n",
    "                                            if 'TripUpdate' in file_name and 'entity' in json_data:\n",
    "                                                for entity in json_data['entity']:\n",
    "                                                    if 'trip_update' in entity and 'stop_time_update' in entity['trip_update']:\n",
    "                                                        stop_time_updates = entity['trip_update']['stop_time_update']\n",
    "                                                        for stop_update in stop_time_updates:\n",
    "                                                            if 'departure' in stop_update and 'stop_id' in stop_update:\n",
    "                                                                process_trip_update(entity, stop_update, file_name, year, month, day, data_TU)\n",
    "                                            elif 'VehiclePosition' in file_name and 'entity' in json_data:\n",
    "                                                process_vehicle_position_route_based(json_data, data_VP, target_lines)\n",
    "\n",
    "    # Save TripUpdate data to CSV\n",
    "    os.makedirs(parsed_TU_directory, exist_ok=True)\n",
    "    for stop_name, rows in data_TU.items():\n",
    "        save_trip_updates_as_csv(rows, stop_name, parsed_TU_directory)\n",
    "        \n",
    "    # Save VehiclePosition data to JSON\n",
    "    os.makedirs(parsed_VP_directory, exist_ok=True)\n",
    "    for stop_name, stop_data in data_VP.items():\n",
    "        save_vehicle_positions_as_json(stop_data, stop_name, parsed_VP_directory)\n",
    "\n",
    "def process_trip_update(entity, stop_update, file_name, year, month, day, data):\n",
    "    stop_id = stop_update['stop_id']\n",
    "    for stop_name, ids in stop_dict.items():\n",
    "        if stop_id in ids:\n",
    "            departure_time = pd.to_datetime(stop_update['departure']['time'], unit='s')\n",
    "            file_time = file_name.split('_')[0].split('T')[1].replace('Z', '').replace('/', ':')\n",
    "            datetime_str = f\"{year}-{month}-{day} {file_time}\"\n",
    "            datetime_obj = datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "            current_time = datetime_obj - timedelta(hours=5)\n",
    "            departure_time_adj = departure_time - timedelta(hours=5)\n",
    "\n",
    "            new_row = {'id': entity['id'],\n",
    "                       'stop_id': stop_id,\n",
    "                       'file_timestamp': datetime_obj,\n",
    "                       'current_time': current_time,\n",
    "                       'departure_time': departure_time,\n",
    "                       'departure_time_adj': departure_time_adj}\n",
    "            data[stop_name].append(new_row)\n",
    "\n",
    "def process_vehicle_position_route_based(json_data, data, target_lines):\n",
    "    for entity in json_data['entity']:\n",
    "        if 'vehicle' in entity and 'trip' in entity['vehicle'] and entity['vehicle']['trip']['route_id'] in target_lines:\n",
    "            route_id = entity['vehicle']['trip']['route_id']\n",
    "            if route_id not in data:\n",
    "                data[route_id] = []\n",
    "            data[route_id].append(entity['vehicle'])\n",
    "\n",
    "def save_trip_updates_as_csv(rows, stop_name, directory):\n",
    "    if rows:  # Check if there are any rows to save\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.sort_values(by='current_time', inplace=True)\n",
    "        csv_file_path = os.path.join(directory, f\"{stop_name}.csv\")\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f\"Saved DataFrame for {stop_name} to {csv_file_path}\")\n",
    "    else:\n",
    "        print(f\"No data to save for {stop_name}.\")\n",
    "\n",
    "\n",
    "def save_vehicle_positions_as_json(stop_data, stop_name, directory):\n",
    "    if stop_data:  # Check if there is any data to save\n",
    "        json_file_path = os.path.join(directory, f\"{stop_name}.json\")\n",
    "        with open(json_file_path, 'w') as json_file:\n",
    "            json.dump(stop_data, json_file, indent=4)\n",
    "        print(f\"Saved JSON data for {stop_name} to {json_file_path}\")\n",
    "    else:\n",
    "        print(f\"No data to save for {stop_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine, Unit\n",
    "\n",
    "# Ensure the output directory exists\n",
    "output_directory = 'temp/parsed_stops'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Load locations.csv and pre-filter\n",
    "def prefilter_locations(locations_csv, stop_dict):\n",
    "    locations_df = pd.read_csv(locations_csv)\n",
    "    unique_stops = [stop for stops in stop_dict.values() for stop in stops]\n",
    "    pattern = '|'.join(unique_stops)\n",
    "    return locations_df[locations_df['loc_name'].str.contains(pattern, regex=True)]\n",
    "\n",
    "# Function to calculate distance between two points\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    return haversine((lat1, lon1), (lat2, lon2), Unit.METERS)\n",
    "\n",
    "# Process Vehicle Position files\n",
    "def process_vehicle_positions(filename, filtered_locations_df, dataframes):\n",
    "    with open(filename, 'r') as file:\n",
    "        vehicles_data = json.load(file)\n",
    "        for vehicle in vehicles_data:\n",
    "            vehicle_pos = vehicle['position']\n",
    "            vehicle_lat = vehicle_pos['latitude']\n",
    "            vehicle_lon = vehicle_pos['longitude']\n",
    "            for _, loc in filtered_locations_df.iterrows():\n",
    "                loc_lat = loc['latitude']\n",
    "                loc_lon = loc['longitude']\n",
    "                if calculate_distance(vehicle_lat, vehicle_lon, loc_lat, loc_lon) <= 3:  # 3 meters threshold\n",
    "                    for stop in dataframes:\n",
    "                        if stop in loc['loc_name']:\n",
    "                            event_data = {\n",
    "                                'timestamp': pd.to_datetime(vehicle['timestamp'], unit='s').tz_localize('UTC').tz_convert('America/New_York'),\n",
    "                                'current_status': vehicle['current_status'],\n",
    "                                'stop_id': vehicle['stop_id'],\n",
    "                                'location': loc['loc_name'],\n",
    "                                'trip_id': vehicle['trip']['trip_id'],\n",
    "                                'bearing': vehicle_pos['bearing']\n",
    "                            }\n",
    "                            dataframes[stop] = pd.concat([dataframes[stop], pd.DataFrame([event_data])], ignore_index=True)\n",
    "                            break\n",
    "\n",
    "# Save DataFrames to CSV\n",
    "def save_dataframes_to_csv(dataframes, directory):\n",
    "    for stop, df in dataframes.items():\n",
    "        if not df.empty:\n",
    "            df.to_csv(f'{directory}/{stop}_events.csv', index=False)\n",
    "\n",
    "# Main function to run the process\n",
    "def main(parsed_VP_directory, locations_csv, stop_dict):\n",
    "    filtered_locations_df = prefilter_locations(locations_csv, stop_dict)\n",
    "    dataframes = {stop: pd.DataFrame() for stop in stop_dict}\n",
    "\n",
    "    for filename in os.listdir(parsed_VP_directory):\n",
    "        if filename.endswith('.json'):\n",
    "            process_vehicle_positions(os.path.join(parsed_VP_directory, filename), filtered_locations_df, dataframes)\n",
    "\n",
    "    save_dataframes_to_csv(dataframes, output_directory)\n",
    "\n",
    "# Example usage\n",
    "stop_dict = {\n",
    "    # Define your stop_dict here\n",
    "}\n",
    "parsed_VP_directory = 'temp/parsed_VP'\n",
    "locations_csv = 'locations.csv'\n",
    "\n",
    "main(parsed_VP_directory, locations_csv, stop_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
